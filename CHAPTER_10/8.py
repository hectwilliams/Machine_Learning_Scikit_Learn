'''
  Q.

    What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff? 

  A. 

    Backpropagation is an algorithm used to compute gradients in a neural network. 

    Backpropagation makes a forward pass for neuron predictions and a reverse pass for error gradients using gradient descent.   

    Reverse-mode autodiff computes derivatives by propagating through the computational graph. Backpropagation uses reverse-mode autodiff during reverse-pass to compute cost/error  gradients.

'''