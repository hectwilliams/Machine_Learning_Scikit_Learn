'''

Q. 

  Name three advantages of the SELU activation function over ReLU?

A.

  - self normalizes outputs 

  - prevents dying graidents 

  - faster convergence 
  

'''